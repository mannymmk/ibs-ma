z
m <- apply(z,2,mean)
s <- apply(z,2,sd)
z <- scale(z,m,s)
distance <- dist(z)
distance
print(distance,digits=3)
#Cluster Dendrogram
hc.c <- hclust(distance)
hc.a <- hclust(distance, method = "average")
plot(hc.a, hang=-1)
member.c <- cutree(hc.c,3)
member.a <- cutree(hc.a,3)
table(member.c,member.a)
aggregate(z, list(member.a),mean)
aggregate(z, list(member.c),mean)
aggregate(utilities[,-c(1,2,12,13,14)]), list(member.c),mean)
aggregate(utilities[,-c(1,2,12,13,14)], list(member.c),mean)
install.packages("cluster")
library(cluster)
plot(silhouette(cutree(hc.a,3), distance))
wss <- (nrow(z)-1)*sum(apply(z,2,var))
for (i in 2:20) wss[i] <- sum(kmeans(z, centers=i)$withinss)
plot(1:20, wss, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares")
# R packages -- "bigdatapackages.R"
# for standard install in Bloomberg Lab
install.packages("AppliedPredictiveModeling")
install.packages("arules")
install.packages("arulesViz")
install.packages("caret", dependencies = c("Depends", "Suggests"))
install.packages("corrplot")
install.packages("dplyr")
install.package("e1071")
install.packages("ElemStatLearn")
install.packages("gcookbook")
install.packages("ggplot2")
install.packages("ggvis")
install.packages("googleVis")
install.packages("httr")
install.packages("knitr")
install.packages("lubridate")
install.packages("manipulate")
install.packages("maps")
install.packages("party")
install.packages("plotly")
install.packages("readr")
install.packages("rggobi")
install.packages("rpart")
install.packages("shiny")
install.packages("SnowballC")
install.packages("tidyr")
install.packages("tm")
# Especially for BUS256 Marketing Analytics
install.packages("car")
install.packages("ChoiceModelR")
install.packages("gam")
install.packages("igraph")
install.packages("nnet")
install.packages("quantmod")
install.packages("Quandl")
install.packages("randomForest")
install.packages("RColorBrewer")
install.packages("RCurl")
install.packages("ROCR")
install.packages("rpart.plot")
install.packages("support.CEs")
install.packages("vcd")
install.packages("wordcloud")
install.packages("XML")
install.packages("xts")
install.packages("zoo")
setwd("C:/Users/Manny/Desktop/BUS256")
load(file="mtpa_spine_chart.Rdata")
print.digits <- 2
library(support.CEs)
provider.survey <- Lma.design(attribute.names =
list(brand = c("AT&T","T-Mobile","US Cellular","Verizon"),
startup = c("$100","$200","$300","$400"),
monthly = c("$100","$200","$300","$400"),
service = c("4G NO","4G YES"),
retail = c("Retail NO","Retail YES"),
apple = c("Apple NO","Apple YES"),
samsung = c("Samsung NO","Samsung YES"),
google = c("Nexus NO","Nexus YES")), nalternatives = 1, nblocks=1, seed=9999)
print(questionnaire(provider.survey))
sink("questions_for_survey.txt")
questionnaire(provider.survey)
sink("questions_for_survey.txt")  # send survey to external text file
questionnaire(provider.survey)
sink()
effect.name.map <- function(effect.name) {
if(effect.name=="brand") return("Mobile Service Provider")
if(effect.name=="startup") return("Start-up Cost")
if(effect.name=="monthly") return("Monthly Cost")
if(effect.name=="service") return("Offers 4G Service")
if(effect.name=="retail") return("Has Nearby Retail Store")
if(effect.name=="apple") return("Sells Apple Products")
if(effect.name=="samsung") return("Sells Samsung Products")
if(effect.name=="google") return("Sells Google/Nexus Products")
}
View(effect.name.map)
conjoint.data.frame <- read.csv
# set up sum contrasts for effects coding as needed for conjoint analysis
options(contrasts=c("contr.sum","contr.poly"))
# main effects model specification
main.effects.model <- {ranking ~ brand + startup + monthly + service +
retail + apple + samsung + google}
# fit linear regression model using main effects only (no interaction terms)
main.effects.model.fit <- lm(main.effects.model, data=conjoint.data.frame)
print(summary(main.effects.model.fit))
# MDS_Exhibit_1_1.R
# Traditional Conjoint Analysis (R)
# adapted for use in BUS256 by Prof. R. Carver, Brandeis University
#
# As usual, set your working directory (modify the next line for your use):
setwd("C:/Users/Manny/Desktop/BUS256")
# R preliminaries to get the user-defined function for spine chart:
# place the spine chart R binary file in your working directory before executing the next statement.
load(file="mtpa_spine_chart.Rdata")
# spine chart accommodates up to 45 part-worths on one page
# |part-worth| <= 40 can be plotted directly on the spine chart
# |part-worths| > 40 can be accommodated through standardization
print.digits <- 2  # set number of digits on print and spine chart
library(support.CEs)  # package for survey construction
# generate a balanced set of product profiles for survey
provider.survey <- Lma.design(attribute.names =
list(brand = c("AT&T","T-Mobile","US Cellular","Verizon"),
startup = c("$100","$200","$300","$400"),
monthly = c("$100","$200","$300","$400"),
service = c("4G NO","4G YES"),
retail = c("Retail NO","Retail YES"),
apple = c("Apple NO","Apple YES"),
samsung = c("Samsung NO","Samsung YES"),
google = c("Nexus NO","Nexus YES")), nalternatives = 1, nblocks=1, seed=9999)
print(questionnaire(provider.survey))  # print survey design for review
sink("questions_for_survey.txt")  # send survey to external text file
questionnaire(provider.survey)
sink() # send output back to the screen
# user-defined function for plotting descriptive attribute names
effect.name.map <- function(effect.name) {
if(effect.name=="brand") return("Mobile Service Provider")
if(effect.name=="startup") return("Start-up Cost")
if(effect.name=="monthly") return("Monthly Cost")
if(effect.name=="service") return("Offers 4G Service")
if(effect.name=="retail") return("Has Nearby Retail Store")
if(effect.name=="apple") return("Sells Apple Products")
if(effect.name=="samsung") return("Sells Samsung Products")
if(effect.name=="google") return("Sells Google/Nexus Products")
}
# read in conjoint survey profiles with respondent ranks
####
#   Attention IBS BUS256:  the next statement has been modified for our use)
####
conjoint.data.frame <- read.csv("data/mobile_services_ranking.csv")
# set up sum contrasts for effects coding as needed for conjoint analysis
options(contrasts=c("contr.sum","contr.poly"))
# main effects model specification
main.effects.model <- {ranking ~ brand + startup + monthly + service +
retail + apple + samsung + google}
# fit linear regression model using main effects only (no interaction terms)
main.effects.model.fit <- lm(main.effects.model, data=conjoint.data.frame)
print(summary(main.effects.model.fit))
# save key list elements of the fitted model as needed for conjoint measures
conjoint.results <-
main.effects.model.fit[c("contrasts","xlevels","coefficients")]
conjoint.results$attributes <- names(conjoint.results$contrasts)
# MDS_Exhibit_1_1.R
# Traditional Conjoint Analysis (R)
# adapted for use in BUS256 by Prof. R. Carver, Brandeis University
#
# As usual, set your working directory (modify the next line for your use):
setwd("C:/Users/Manny/Desktop/BUS256")
# R preliminaries to get the user-defined function for spine chart:
# place the spine chart R binary file in your working directory before executing the next statement.
load(file="mtpa_spine_chart.Rdata")
# spine chart accommodates up to 45 part-worths on one page
# |part-worth| <= 40 can be plotted directly on the spine chart
# |part-worths| > 40 can be accommodated through standardization
print.digits <- 2  # set number of digits on print and spine chart
library(support.CEs)  # package for survey construction
# generate a balanced set of product profiles for survey
provider.survey <- Lma.design(attribute.names =
list(brand = c("AT&T","T-Mobile","US Cellular","Verizon"),
startup = c("$100","$200","$300","$400"),
monthly = c("$100","$200","$300","$400"),
service = c("4G NO","4G YES"),
retail = c("Retail NO","Retail YES"),
apple = c("Apple NO","Apple YES"),
samsung = c("Samsung NO","Samsung YES"),
google = c("Nexus NO","Nexus YES")), nalternatives = 1, nblocks=1, seed=9999)
print(questionnaire(provider.survey))  # print survey design for review
sink("questions_for_survey.txt")  # send survey to external text file
questionnaire(provider.survey)
sink() # send output back to the screen
# user-defined function for plotting descriptive attribute names
effect.name.map <- function(effect.name) {
if(effect.name=="brand") return("Mobile Service Provider")
if(effect.name=="startup") return("Start-up Cost")
if(effect.name=="monthly") return("Monthly Cost")
if(effect.name=="service") return("Offers 4G Service")
if(effect.name=="retail") return("Has Nearby Retail Store")
if(effect.name=="apple") return("Sells Apple Products")
if(effect.name=="samsung") return("Sells Samsung Products")
if(effect.name=="google") return("Sells Google/Nexus Products")
}
# read in conjoint survey profiles with respondent ranks
####
#   Attention IBS BUS256:  the next statement has been modified for our use)
####
conjoint.data.frame <- read.csv("data/mobile_services_ranking.csv")
# set up sum contrasts for effects coding as needed for conjoint analysis
options(contrasts=c("contr.sum","contr.poly"))
# main effects model specification
main.effects.model <- {ranking ~ brand + startup + monthly + service +
retail + apple + samsung + google}
# fit linear regression model using main effects only (no interaction terms)
main.effects.model.fit <- lm(main.effects.model, data=conjoint.data.frame)
print(summary(main.effects.model.fit))
# save key list elements of the fitted model as needed for conjoint measures
conjoint.results <-
main.effects.model.fit[c("contrasts","xlevels","coefficients")]
conjoint.results$attributes <- names(conjoint.results$contrasts)
# compute and store part-worths in the conjoint.results list structure
part.worths <- conjoint.results$xlevels  # list of same structure as xlevels
end.index.for.coefficient <- 1  # intitialize skipping the intercept
part.worth.vector <- NULL # used for accumulation of part worths
for(index.for.attribute in seq(along=conjoint.results$contrasts)) {
nlevels <- length(unlist(conjoint.results$xlevels[index.for.attribute]))
begin.index.for.coefficient <- end.index.for.coefficient + 1
end.index.for.coefficient <- begin.index.for.coefficient + nlevels -2
last.part.worth <- -sum(conjoint.results$coefficients[
begin.index.for.coefficient:end.index.for.coefficient])
part.worths[index.for.attribute] <-
list(as.numeric(c(conjoint.results$coefficients[
begin.index.for.coefficient:end.index.for.coefficient],
last.part.worth)))
part.worth.vector <-
c(part.worth.vector,unlist(part.worths[index.for.attribute]))
}
conjoint.results$part.worths <- part.worths
# compute standardized part-worths
standardize <- function(x) {(x - mean(x)) / sd(x)}
conjoint.results$standardized.part.worths <-
lapply(conjoint.results$part.worths,standardize)
# compute and store part-worth ranges for each attribute
part.worth.ranges <- conjoint.results$contrasts
for(index.for.attribute in seq(along=conjoint.results$contrasts))
part.worth.ranges[index.for.attribute] <-
dist(range(conjoint.results$part.worths[index.for.attribute]))
conjoint.results$part.worth.ranges <- part.worth.ranges
sum.part.worth.ranges <- sum(as.numeric(conjoint.results$part.worth.ranges))
# compute and store importance values for each attribute
attribute.importance <- conjoint.results$contrasts
for(index.for.attribute in seq(along=conjoint.results$contrasts))
attribute.importance[index.for.attribute] <-
(dist(range(conjoint.results$part.worths[index.for.attribute]))/
sum.part.worth.ranges) * 100
conjoint.results$attribute.importance <- attribute.importance
# data frame for ordering attribute names
attribute.name <- names(conjoint.results$contrasts)
attribute.importance <- as.numeric(attribute.importance)
temp.frame <- data.frame(attribute.name,attribute.importance)
conjoint.results$ordered.attributes <-
as.character(temp.frame[sort.list(
temp.frame$attribute.importance,decreasing = TRUE),"attribute.name"])
# respondent internal consistency added to list structure
conjoint.results$internal.consistency <- summary(main.effects.model.fit)$r.squared
# user-defined function for printing conjoint measures
if (print.digits == 2)
pretty.print <- function(x) {sprintf("%1.2f",round(x,digits = 2))}
if (print.digits == 3)
pretty.print <- function(x) {sprintf("%1.3f",round(x,digits = 3))}
# report conjoint measures to console
# use pretty.print to provide nicely formated output
for(k in seq(along=conjoint.results$ordered.attributes)) {
cat("\n","\n")
cat(conjoint.results$ordered.attributes[k],"Levels: ",
unlist(conjoint.results$xlevels[conjoint.results$ordered.attributes[k]]))
cat("\n"," Part-Worths:  ")
cat(pretty.print(unlist(conjoint.results$part.worths
[conjoint.results$ordered.attributes[k]])))
cat("\n"," Standardized Part-Worths:  ")
cat(pretty.print(unlist(conjoint.results$standardized.part.worths
[conjoint.results$ordered.attributes[k]])))
cat("\n"," Attribute Importance:  ")
cat(pretty.print(unlist(conjoint.results$attribute.importance
[conjoint.results$ordered.attributes[k]])))
}
# plotting of spine chart begins here
# all graphical output is routed to external pdf file
pdf(file = "fig_preference_mobile_services_results.pdf", width=8.5, height=11)
# Nows produce the graph from Exh 1.1
spine.chart(conjoint.results)
dev.off()  # close the graphics output device
# Look in your working directory, where you should now find
# a new pdf file called "fig_preference_mobile_services_results.pdf". Open it.
# Further Suggestions for the student:
# Enter your own rankings for the product profiles and generate
# conjoint measures of attribute importance and level part-worths.
# Note that the model fit to the data is a linear main-effects model.
# See if you can build a model with interaction effects for service
# provider attributes.
View(conjoint.data.frame)
View(conjoint.data.frame)
load("C:/Users/Manny/Downloads/email-failures-2017-february/email-failures-2017-february/failure-report.csv")
library(readr)
failure_report <- read_csv("C:/Users/Manny/Downloads/email-failures-2017-february/email-failures-2017-february/failure-report.csv")
View(failure_report)
read.csv(failure_report)
read.csv(failure_report)
order(failure_report)
# Competitive Intelligence: Spirit Airlines Financial Dossier (R)
# install required packages
# bring packages into the workspace
library(RCurl)  # functions for gathering data from the web
library(XML)  # XML and HTML parsing
library(quantmod) # use for gathering and charting economic data
# online documentation for quantmod at <http://www.quantmod.com/>
library(Quandl)  # extensive financial data online
# online documentation for Quandl at <http://www.quandl.com/>
library(lubridate) # date functions
library(zoo)  # utilities for working with time series
library(xts)  # utilities for working with time series
library(ggplot2)  # data visualization
getSymbols("COF", return.class = "xts", src = "yahoo")
print(str(COF))
View(COF)
View(COF)
#get COF stock price data
getSymbols("COF", return.class = "xts", src = "yahoo")
print(str(COF)) #show the structure of this xts time series object
#plot the series stock price
chartSeries(COF,theme="white")
# examine the structure of the R data object
print(str(COF))
print(COF)
#Q1
# get Capital One Finance (COF) stock price data
getSymbols("COF", return.class = "xts", src = "yahoo")
print(str(COF)) # show the structure of this xtx time series object
# plot the series stock price
chartSeries(COF,theme="white")
# examine the structure of the R data object
print(str(COF))
print(COF)
# convert character string row names to decimal date for the year
Year <- decimal_date(ymd(row.names(as.data.frame(COF))))
# Obtain the closing price of Capitol One (COF) stock
Price <- as.numeric(COF$COF.Close)
# create data frame for Capitol One (COF) Year and Price for future plots
COF_data_frame <- data.frame(Year, Price)
# -----------------------------------------
# ggplot2 time series plotting, closing
# price of Capitol One Financial (COF) common stock
# -----------------------------------------
# with a data frame object in hand... we can go on to use ggplot2
# and methods described in Chang (2013) R Graphics Cookbook
# use data frame defined from Yahoo! Finance
# the Capitol One Financial (COF) closing price per share COF_data_frame
# now we use that data structure to prepare a time series plot
# again, let's highlight the Great Recession on our plot for this
# time series
plotting_object <- ggplot(COF_data_frame, aes(x = Year, y = Price)) +
geom_line() +
ylab("Stock Price (dollars/share)") +
ggtitle("Capitol One Financial (COF) Stock Price")
print(plotting_object)
# send the plot to an external file (sans title, larger axis labels)
pdf("fig_competitive_intelligence_COF.pdf", width = 11, height = 8.5)
plotting_object <- ggplot(COF_data_frame, aes(x = Year, y = Price)) +
geom_line() + ylab("Stock Price (dollars/share)") +
theme(axis.title.y = element_text(size = 20, colour = "black")) +
theme(axis.title.x = element_text(size = 20, colour = "black"))
print(plotting_object)
dev.off()
#Q1C2
cof_url <- "http://chart.finance.yahoo.com/table.csv?s=COF&a=0&b=1&c=2000&d=1&e=26&f=2017&g=d&ignore=.csv"
yahoo.read <- function(url){
dat <- read.table(url,header=TRUE,sep=",")
df <- dat[,c(1,5)]
df$Date <- as.Date(as.character(df$Date))
return(df)}
cof <- yahoo.read(cof_url)
ggplot(cof,aes(Date,Close)) +
geom_line(aes()) +
ylab("Stock Price (dollars/share)") +
ggtitle("Capitol One Financial (COF) Stock Price")
cof_url <- "http://chart.finance.yahoo.com/table.csv?s=COF&a=0&b=1&c=2000&d=1&e=26&f=2017&g=d&ignore=.csv"
axp_url <- "http://chart.finance.yahoo.com/table.csv?s=AXP&a=0&b=1&c=2000&d=1&e=26&f=2017&g=d&ignore=.csv"
dfs_url <- "http://chart.finance.yahoo.com/table.csv?s=DFS&a=0&b=1&c=2000&d=1&e=26&f=2017&g=d&ignore=.csv"
syf_url <- "http://chart.finance.yahoo.com/table.csv?s=SYF&a=0&b=1&c=2000&d=1&e=26&f=2017&g=d&ignore=.csv"
yahoo.read <- function(url){
dat <- read.table(url,header=TRUE,sep=",")
df <- dat[,c(1,5)]
df$Date <- as.Date(as.character(df$Date))
return(df)}
cof <- yahoo.read(cof_url)
axp <- yahoo.read(axp_url)
dfs <- yahoo.read(dfs_url)
syf <- yahoo.read(syf_url)
ggplot(cof,aes(Date,Close)) +
geom_line(aes(color="cof")) +
geom_line(data=axp,aes(color="axp")) +
geom_line(data=dfs,aes(color="dfs")) +
geom_line(data=syf,aes(color="syf")) +
ylab("Stock Price (dollars/share)") +
xlab("Year") +
ggtitle("Comparable Stock Prices")
# Comparing National Civilian Unemployment Rate to COF Stock Price
library(ggplot2)
library(gtable)
library(grid)
library(extrafont)
# National Civilian Unemployment Rate,
#    not seasonally adjusted (monthly, percentage)
getSymbols("UNRATENSA",src="FRED",return.class = "xts")
print(str(UNRATENSA)) # show the structure of this xtx time series object
# plot the series
chartSeries(UNRATENSA,theme="white")
UNRATENSA_DATA <- data.frame(Date = index(UNRATENSA),
UNRATENSA, row.names=NULL)
UnemploySubset <- UNRATENSA_DATA[UNRATENSA_DATA$Date > as.Date("1999-12-01"),]
library("gtable")
cof$normclose <- cof$Close/46.1875
UnemploySubset$normclose <- UnemploySubset$UNRATENSA/4.5
ggplot(cof,aes(Date,normclose)) +
geom_line(aes(color="cof")) +
geom_line(data=UnemploySubset,aes(color="UNRATENSA")) +
ylab("Comparative Levels") +
xlab("Year") +
ggtitle("COF vs UNRATENSA")
str(COF)
View(cof)
View(UnemploySubset)
View(UnemploySubset)
lm(cof$normclose ~ UnemploySubset$normclose, na.action = "na.exclude")
str(COF)
str(UnemploySubset)
str(cof)
print(str(col))
print(head(col))
library(xts)
ts <- xts(cof$normclose, as.Date(cof$Date, "%Y-%m-%d"))
# convert daily data
ts_m = apply.monthly(ts, mean)
ts_m$avgclose <- ts_m$V1
ts_m_DATA <- data.frame(Date = index(ts_m),
ts_m, row.names=NULL)
tsmSubset <- ts_m_DATA[ts_m_DATA$Date < as.Date("2017-2-01"),]
new <- data.frame(tsmSubset$ts_m, UnemploySubset$normclose)
model <- lm(new$tsmSubset.ts_m ~ new$UnemploySubset.normclose)
print(model)
plot(model)
fit <- ets(myts)
forecast(fit, 1)
library("forecast", lib.loc="~/R/win-library/3.3")
library(forecast)
myts <- ts(Revenue_1$`Revenue (Billion)`)
fit <- ets(myts)
forecast(fit, 1)
library(forecast)
myts <- ts(Revenue_1$`Revenue (Billion)`)
library(readr)
Revenue_1 <- read_csv("C:/Users/Manny/Downloads/Revenue-1.csv",
col_types = cols(Date = col_date(format = "%m/%d/%Y"),
`Revenue (Billion)` = col_number()))
View(Revenue_1)
library(forecast)
myts <- ts(Revenue_1$`Revenue (Billion)`)
fit <- ets(myts)
forecast(fit, 1)
fit <- ets(myts)
forecast(fit, 1)
library("forecast", lib.loc="~/R/win-library/3.3")
View(failure_report)
View(failure_report)
library(readr)
Revenue_1 <- read_csv("C:/Users/Manny/Downloads/Revenue-1.csv",
col_types = cols(Date = col_date(format = "%m/%d/%Y"),
`Revenue (Billion)` = col_number()))
View(Revenue_1)
library(forecast)
myts <- ts(Revenue_1$`Revenue (Billion)`)
fit <- ets(myts)
forecast(fit, 1)
library(forecast)
myts <- ts(Revenue_1$`Revenue (Billion)`)
fit <- ets(myts)
forecast(fit, 1)
install.packages('rvest')
install.packages('stringr')
install.packages('tidyr')
library(rvest)
library(stringr)
library(tidyr)
cof <- html("https://www.indeed.com/cmp/Capital-One/jobs")
cof %>%
html_nodes(".cmp-job-entry") %>%
html_text()
library(forecast)
myts <- ts(Revenue_1$`Revenue (Billion)`)
fit <- ets(myts)
forecast(fit, 1)
install.packages("forecast")
library(forecast)
myts <- ts(Revenue_1$`Revenue (Billion)`)
fit <- ets(myts)
forecast(fit, 1)
library(arules)  # association rules
library(arulesViz)  # data visualization of association rules
library(RColorBrewer)  # color palettes for plots
setwd("~/ibs-ma")
